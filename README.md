# âœ‹ Hand Gesture Volume Control using AI ğŸ¤–ğŸšï¸

## ğŸ“Œ Overview
This project is an **AI-powered Hand Gesture Volume Control System** that utilizes **MediaPipe for real-time hand tracking** and **Pycaw for audio control**. By simply moving your fingers, you can **increase, decrease, or mute the system volume**â€”all without touching the keyboard! ğŸš€

## ğŸ” How It Works
1. **Hand landmarks** are detected using **MediaPipe Hands**.
2. The distance between **thumb & index finger** is calculated to control volume.
3. **Pycaw** is used to change the system's master volume based on the hand gesture.
4. The **FPS (frames per second)** is calculated and displayed for performance tracking.
5. **Real-time visualization** with OpenCV to enhance user experience. ğŸ¥

## âœ‹ Hand Detection Logic
The **MediaPipe Hands API** detects **21 key landmarks** on the hand, which helps in gesture recognition.
```python
mp_hands = mp.solutions.hands
with mp_hands.Hands(
    model_complexity=0,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7) as hands:
```

Once detected, **specific landmarks (thumb & index finger)** are extracted:
```python
x1, y1 = lmList[4][1], lmList[4][2]  # Thumb Tip
x2, y2 = lmList[8][1], lmList[8][2]  # Index Finger Tip
```

A line is drawn between them for better visualization:
```python
cv2.line(image, (x1, y1), (x2, y2), (255, 0, 255), 3)  # Pink Line
```

## ğŸ§  Logic Behind Volume Control & Other Libraries
### ğŸ¯ **MediaPipe Hand Tracking**
- **Detects hands** in real-time and provides **landmark positions**.
- **Tracks hand movements**, ensuring smooth volume control.

### ğŸ”Š **Pycaw for Volume Control**
- Extracts the system's **audio control interface**:
```python
devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
volume = cast(interface, POINTER(IAudioEndpointVolume))
```
- Maps **finger distance** to volume levels:
```python
vol = np.interp(length, [50, 220], [minVol, maxVol])
volume.SetMasterVolumeLevel(vol, None)
```

### ğŸ“ **Mathematics Behind Volume Scaling**
- **Hypotenuse formula** is used to calculate the finger distance:
```python
length = math.hypot(x2 - x1, y2 - y1)
```
- The volume bar dynamically adjusts based on the finger position:
```python
volBar = np.interp(length, [50, 220], [400, 150])
volPer = np.interp(length, [50, 220], [0, 100])
```

## ğŸ› ï¸ Code Breakdown
### **1ï¸âƒ£ Initializing Libraries & Webcam**
```python
cam = cv2.VideoCapture(0)
cam.set(3, 640)  # Width
cam.set(4, 480)  # Height
```

### **2ï¸âƒ£ Detecting Hand Landmarks**
```python
results = hands.process(image)
if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        mp_drawing.draw_landmarks(
            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
```

### **3ï¸âƒ£ Controlling Volume Based on Finger Distance**
```python
if len(lmList) != 0:
    length = math.hypot(x2 - x1, y2 - y1)
    vol = np.interp(length, [50, 220], [minVol, maxVol])
    volume.SetMasterVolumeLevel(vol, None)
```

### **4ï¸âƒ£ Displaying Volume Bar & FPS**
```python
cv2.rectangle(image, (50, int(volBar)), (85, 400), (0, 255, 0), cv2.FILLED)
cv2.putText(image, f'{int(volPer)} %', (40, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 3)
```

## ğŸ“¦ Requirements
Before running the project, install the necessary dependencies:
```bash
pip install opencv-python mediapipe numpy pycaw comtypes
```

## ğŸš€ How to Run
1. **Clone the repository**:
   ```bash
   git clone https://github.com/debanganghosh08/hand_gesture_volume_control.git
   cd hand_gesture_volume_control
   ```
2. **Run the script**:
   ```bash
   main.py
   ```
3. **Use hand gestures** to control system volume! ğŸ–ï¸ğŸšï¸

## ğŸ¯ Future Enhancements
ğŸ”¹ Add **gesture support for play/pause music** ğŸµ  
ğŸ”¹ Improve **gesture recognition for multi-hand control** âœ‹ğŸ¤š  
ğŸ”¹ Implement **AI-based hand gesture prediction** ğŸ§   

ğŸ“¢ **Letâ€™s collaborate!** Fork the repo, experiment, and share your ideas. ğŸš€


